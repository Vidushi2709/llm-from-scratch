#  Build LLM à¼¼ ã¤ â—•_â—• à¼½ã¤


A hands-on repository where I tinker, train, and transform language models from the ground up. Imagine it as a playground for neural networks, where code meets creativity! ğŸ§©ğŸ¤–âœ¨

â˜†*: .ï½¡. o(â‰§â–½â‰¦)o .ï½¡.:*â˜†

---

##  What Youâ€™ll Explore (*/Ï‰ï¼¼*)

```text
âŒ¨ï¸ Tokenization Experiments
   Turn text â†’ numbers â†’ magic. Watch your words get "vectorized"!

ğŸ”„ Transformers From Scratch
   - Self-Attention (because every token deserves to be heard)
   - Multi-Head Attention (like gossiping, but mathematically)
   - Feed-Forward Layers (aka the brain's gym)
   - Layer Normalization (keeping neurons calm under pressure)

ğŸ“ Mini GPT Models
   Tiny models with big dreamsâ€”they try their best to talk back.

ğŸ’ƒ Gradient Gymnastics
   Watch gradients twist, turn, and pirouette during training.

ğŸ•µï¸â€â™‚ï¸ Parameter & Memory Detective Work
   Peek under the hood: because yes, model size *does* matter.
```

---

## Getting Started â•°(*Â°â–½Â°*)â•¯

### 1ï¸âƒ£ Clone the Repo

```bash
git clone https://github.com/yourusername/Build-LLM-Playground.git
cd Build-LLM-Playground
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install torch tiktoken transformers
```

### 3ï¸âƒ£ Run Experiments

```bash
python ch3_gpt_model/main.py
```

---

## Notes (âÂ´â—¡`â)

```text
âš ï¸ These models are tiny.
   They wonâ€™t take over the worldâ€¦ yet! ğŸ˜

ğŸ” Some experiments compare against Hugging Face models.
   Goal: learn-by-building, not just copying.

ğŸ’¡ Expect bugs, unexpected outputs, and occasional "why is this happening?!" moments.
   Thatâ€™s all part of the fun!

ğŸ˜ Contributions, ideas, memes â†’ always welcome.
```

---

## Author (â”¬â”¬ï¹â”¬â”¬)

**Vin** â€“ a curious tinkerer exploring the inner workings of LLMs, one line of code at a time.

