{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b315f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from ch3 import MultiHeadAttention\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2536cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,    # Vocabulary size\n",
    "        \"context_length\": 1024, # Context length\n",
    "        \"emb_dim\": 768,         # Embedding dimension\n",
    "        \"n_heads\": 12,          # Number of attention heads\n",
    "        \"n_layers\": 12,         # Number of layers\n",
    "        \"drop_rate\": 0.1,       # Dropout rate\n",
    "        \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6928876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c59a01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "                *[DummyTransformerBlock(cfg)\n",
    "                  for _ in range(cfg[\"n_layers\"])]\n",
    "                )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "                cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "                )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "                torch.arange(seq_len, device=in_idx.device)\n",
    "                )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad643dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # unbiased=False matches nn.LayerNorm\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        return self.gamma * (x - mean) / torch.sqrt(var + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ea6eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b9b48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # 768 → 3072\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),  # 3072 → 768\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4001e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "                # Implement 5 layers\n",
    "\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "                ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "# A function to compute gradients\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # Backward pass to calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absoute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "100c8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "         d_in=cfg[\"emb_dim\"],\n",
    "         d_out=cfg[\"emb_dim\"],          \n",
    "         context_length=cfg[\"context_length\"],\n",
    "         dropout=cfg[\"drop_rate\"],\n",
    "         num_heads=cfg[\"n_heads\"],\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head attention + residual\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Feed-forward + residual\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "877c80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockSeperateDropout(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn= MultiHeadAttention(\n",
    "                    d_in=cfg[\"emb_dim\"],\n",
    "                    d_out=cfg[\"emb_dim\"],\n",
    "                    context_length=cfg[\"context_length\"],\n",
    "                    num_heads=cfg[\"n_heads\"],\n",
    "                    dropout=cfg[\"attn_drop_rate\"],\n",
    "                    qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff= FeedForward(cfg)\n",
    "        self.norm1= LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2= LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut= nn.Dropout(cfg[\"shortcut_drop_rate\"])\n",
    "     \n",
    "    def forward(self,x):\n",
    "         shortcut= x\n",
    "         x= self.norm1(x)\n",
    "         x = self.attn(x)\n",
    "         x = x+ shortcut\n",
    "\n",
    "         shortcut=x\n",
    "         x= self.norm2(x)\n",
    "         x= self.attn(x)\n",
    "         x=x+shortcut\n",
    "         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ffed4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb= nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb= nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb= nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks= nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "\n",
    "        self.final_norm= LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head= nn.Linear(\n",
    "            cfg['emb_dim'], cfg['vocab_size'], bias= False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds= self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds= self.pos_emb(\n",
    "            torch.arange(seq_len, device= in_idx.device)\n",
    "        )\n",
    "\n",
    "        x= tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits= self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "26a5895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelAndCalculateSize(conf):\n",
    "    model= GPTModel(conf)\n",
    "    total_params= sum(p.numel() for p in model.parameters())\n",
    "    total_size_in_bytes= total_params*4\n",
    "    total_size_in_mbs= total_size_in_bytes/(1024*1024)\n",
    "    return total_params, total_size_in_mbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f77785cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_text(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond= idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits= model(idx_cond)\n",
    "        logits= logits[:,-1,:]\n",
    "        probas= torch.softmax(logits, dim=-1)\n",
    "        idx_next= torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx= torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a53add3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "    batch=[]\n",
    "    txt1= \"Everybody makes\"\n",
    "    txt2=\"Everyday hold a\"\n",
    "\n",
    "    batch = [torch.tensor(tokenizer.encode(txt)) for txt in [txt1, txt2]]\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    print(batch)\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model=DummyGPTModel(GPT_CONFIG_124M)\n",
    "    logits= model(batch)\n",
    "    print(\"Output shape:\", logits.shape)\n",
    "    print(logits)\n",
    "\n",
    "    # Layer Normalization\n",
    "    torch.manual_seed(123)\n",
    "    batch_example= torch.randn(2,5)\n",
    "    layer= nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "    out= layer(batch_example)\n",
    "    print(out)\n",
    "\n",
    "    mean=out.mean(dim=-1, keepdim=True)\n",
    "    var= out.var(dim=-1, unbiased=False, keepdim=True)\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Variance:\", var)\n",
    "\n",
    "    out_norm = (out - mean) / torch.sqrt(var + 1e-5)\n",
    "    mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "    var = out_norm.var(dim=-1, keepdim=True)\n",
    "    print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "    print(\"Mean:\\n\" , mean)\n",
    "    print(\"Variance:\\n\", var)\n",
    "\n",
    "    torch.set_printoptions(sci_mode=False)\n",
    "    print(\"Mean:\\n\" , mean)\n",
    "    print(\"Variance:\\n\", var)\n",
    "\n",
    "    # Trying LayerNorm in practice\n",
    "    \n",
    "    ln = LayerNorm(emb_dim=5)\n",
    "    out_ln = ln(batch_example)\n",
    "    mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "    var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "    print(\"Mean:\\n\", mean)\n",
    "    print(\"Variance:\\n\", var)\n",
    "\n",
    "\n",
    "    ffn = FeedForward(GPT_CONFIG_124M)\n",
    "    x = torch.rand(2, 3, 768)\n",
    "    out = ffn(x)\n",
    "    print(out.shape)\n",
    "\n",
    "\n",
    "    # First we implement a neural net without shortcut connections\n",
    "    \n",
    "    layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "    sample_input = torch.tensor([[1.0, 0., -1.]])\n",
    "    torch.manual_seed(123)\n",
    "    model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "\n",
    "\n",
    "    print(\"Model gradients without shortcut:\")\n",
    "    print_gradients(model_without_shortcut, sample_input)\n",
    "    \n",
    "    # Now to compare with a model that has gradients\n",
    "    torch.manual_seed(123)\n",
    "    model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "            layer_sizes, use_shortcut=True\n",
    "            )\n",
    "    print(\"Model gradients with shortcuts:\")\n",
    "    print_gradients(model_with_shortcut, sample_input)\n",
    "\n",
    "\n",
    "    # Instantiating transformer block and feeding it some sample data\n",
    "    torch.manual_seed(123)\n",
    "    x = torch.rand(2, 4, 768)\n",
    "    block = TransformerBlock(GPT_CONFIG_124M)\n",
    "    output = block(x)\n",
    "    \n",
    "    print(\"Transfrormer input shape:\", x.shape)\n",
    "    print(\"Transformer output shape:\", output.shape)\n",
    "\n",
    "    # Sample batch to our GPT model\n",
    "    \n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    \n",
    "    out = model(batch)\n",
    "    print(\"Input batch:\\n\", batch)\n",
    "    print(\"\\nOutput shape:\", out.shape)\n",
    "    print(out)\n",
    "    \n",
    "    # Analyzing the size of the model we coded up earlier\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "    \n",
    "    print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "    print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "    \n",
    "    total_params_gpt2 = (\n",
    "            total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "            )\n",
    "    print(f\"Number of trainable parameters \"\n",
    "          f\"considering weight tying: {total_params_gpt2:,}\")\n",
    "    \n",
    "    # Feed forward module and multi-head attention module amount of parameters\n",
    "    one_of_transformers = model.trf_blocks[0]\n",
    "    feed_forward = one_of_transformers.ff\n",
    "    attention = one_of_transformers.att\n",
    "    \n",
    "    feed_forward_params = sum(p.numel() for p in feed_forward.parameters())\n",
    "    attention_params = sum(p.numel() for p in attention.parameters())\n",
    "    \n",
    "    print(f\"Feed forward has {feed_forward_params:,} trainable weights\")\n",
    "    print(f\"Attention has {attention_params:,} trainable weights\")\n",
    "    \n",
    "    # Assesing memmory requirements\n",
    "    total_size_bytes = total_params * 4\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024) # Convert to Megabytes\n",
    "    print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Exercise 4.2 Calculating number of parameters and require memmory for GPT-2 medium, GPT-2 large and GPT-2 XL\n",
    "    \n",
    "    GPT_CONFIGS = {\n",
    "        \"GPT-2 medium\": {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1024,         # Embedding dimension\n",
    "            \"n_heads\": 16,          # Number of attention heads\n",
    "            \"n_layers\": 24,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        },\n",
    "        \"GPT-2 large\": {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1280,         # Embedding dimension\n",
    "            \"n_heads\": 20,          # Number of attention heads\n",
    "            \"n_layers\": 36,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        },\n",
    "        \"GPT-2 XL\": {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 1600,         # Embedding dimension\n",
    "            \"n_heads\": 25,          # Number of attention heads\n",
    "            \"n_layers\": 48,         # Number of layers\n",
    "            \"drop_rate\": 0.1,       # Dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    # Lets try it out\n",
    "    start_context = \"Everyone is \"\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    print(\"encoded:\", encoded)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Adds batch dimension\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    \n",
    "    model.eval() # Puts model into eval state to disable random components such as dropout and etc\n",
    "    out = Generate_text(\n",
    "            model=model,\n",
    "            idx=encoded_tensor,\n",
    "            max_new_tokens=10,\n",
    "            context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "            )\n",
    "    print(\"Output:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    \n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    print(decoded_text)\n",
    "\n",
    "    # Exercise 4.3 Using separate dropout parameters\n",
    "    \n",
    "    GPT_CONFIG_124M_SEPARATE_DROOPOUTS = {\n",
    "            \"vocab_size\": 50257,    # Vocabulary size\n",
    "            \"context_length\": 1024, # Context length\n",
    "            \"emb_dim\": 768,         # Embedding dimension\n",
    "            \"n_heads\": 12,          # Number of attention heads\n",
    "            \"n_layers\": 12,         # Number of layers\n",
    "            \"emb_drop_rate\": 0.1,       # Embeddings dropout rate\n",
    "            \"shortcut_drop_rate\": 0.1,  # Shortcut dropout rate\n",
    "            \"att_drop_rate\": 0.1,       # Attention dropout rate\n",
    "            \"qkv_bias\": False       # Query-Key-Value bias\n",
    "    }\n",
    "    \n",
    "    class GPTModelSeparateDropoutParameters(nn.Module):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__()\n",
    "          self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "          self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "          self.drop_emb = nn.Dropout(cfg[\"emb_drop_rate\"])\n",
    "\n",
    "          self.trf_blocks = nn.Sequential(\n",
    "              *[TransformerBlockSeperateDropout(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "          )\n",
    "\n",
    "          self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "          self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "      def forward(self, in_idx):\n",
    "          batch_size, seq_len = in_idx.shape\n",
    "\n",
    "          # Token embeddings\n",
    "          tok_embeds = self.tok_emb(in_idx)  # (batch, seq_len, emb_dim)\n",
    "\n",
    "          # Positional embeddings (expand to batch)\n",
    "          pos_idx = torch.arange(seq_len, device=in_idx.device).unsqueeze(0)  # (1, seq_len)\n",
    "          pos_embeds = self.pos_emb(pos_idx)  # (1, seq_len, emb_dim)\n",
    "          pos_embeds = pos_embeds.expand(batch_size, seq_len, -1)  # broadcast to batch\n",
    "\n",
    "          x = tok_embeds + pos_embeds\n",
    "          x = self.drop_emb(x)\n",
    "          x = self.trf_blocks(x)  # pass through all transformer blocks\n",
    "          x = self.final_norm(x)\n",
    "          logits = self.out_head(x)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "          return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b5780db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28172,  1838,     0,     0],\n",
      "        [ 6109,   820,  1745,   257]])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.4662, -0.4357, -0.5804,  ..., -0.4822,  0.1343, -0.6500],\n",
      "         [-0.9326, -0.2487, -1.8642,  ...,  0.0958, -0.1139,  0.9603],\n",
      "         [ 0.1749,  1.1411,  0.9494,  ...,  0.5431,  0.5032, -0.5057],\n",
      "         [-0.1238,  0.7459,  0.9899,  ...,  1.9023, -0.0482, -0.3757]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-1.1791,  0.1868, -0.5684,  ..., -0.5918,  0.4412,  0.5210],\n",
      "         [ 0.5525,  0.4192, -0.1347,  ...,  0.3754,  0.2626, -0.5079],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[0.0192],\n",
      "        [0.0332]], grad_fn=<VarBackward0>)\n",
      "Normalized layer outputs:\n",
      " tensor([[ 0.6745,  1.5470, -0.9549,  0.6431, -0.9549, -0.9549],\n",
      "        [-0.0207,  0.1228, -1.1913,  1.6619,  0.6186, -1.1913]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.1994],\n",
      "        [1.1996]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.1994],\n",
      "        [1.1996]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "torch.Size([2, 3, 768])\n",
      "Model gradients without shortcut:\n",
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n",
      "Model gradients with shortcuts:\n",
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n",
      "Transfrormer input shape: torch.Size([2, 4, 768])\n",
      "Transformer output shape: torch.Size([2, 4, 768])\n",
      "Input batch:\n",
      " tensor([[28172,  1838,     0,     0],\n",
      "        [ 6109,   820,  1745,   257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.0096,  0.2174,  0.4617,  ..., -0.2830,  0.4158, -0.4765],\n",
      "         [ 0.2802, -0.7264,  0.4574,  ..., -0.6130,  0.1551,  0.4510],\n",
      "         [ 0.8165,  0.9023,  0.3618,  ..., -0.7728, -0.1183, -0.3959],\n",
      "         [-0.6644,  0.9821,  0.6229,  ...,  0.1412, -0.0962, -0.4315]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.0069,  0.5420,  0.2037,  ..., -0.1580,  0.7187, -0.3608],\n",
      "         [ 1.3345,  0.0055, -0.4733,  ...,  0.0926, -0.2374, -0.3725],\n",
      "         [-0.1780,  0.2201,  0.5120,  ...,  1.1818, -0.4301,  0.0161]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Total number of parameters: 163,009,536\n",
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Feed forward has 4,722,432 trainable weights\n",
      "Attention has 2,360,064 trainable weights\n",
      "Total size of the model: 621.83 MB\n",
      "encoded: [16190, 318, 220]\n",
      "encoded_tensor.shape: torch.Size([1, 3])\n",
      "Output: tensor([[16190,   318,   220,  8116, 19141, 37028, 30099, 45767,  9650, 18938,\n",
      "         37860, 32435,  9954]])\n",
      "Output length: 13\n",
      "Everyone is lers Prep ZerFairDistanceotion301 dyn deemelve\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vidushi\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1915: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbf449cf2524dda8717f46b35a96c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vidushi\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Vidushi\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25d35792dcf4535b2e55787c223898f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130e364ca6164c91823557d3d12146a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0ac0074f214a889a74bad05a57ac4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e43243b4e444f80f6bd6c03785039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vidushi\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:3513: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4447ad855341f6bb5aa59099385df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab50cff7711d4d6a8c81983e1587a7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " Everyone is  a little bit of a jerk, but I'm not a jerk. I'm a jerk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Hugging Face token\n",
    "token = \"\"\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", use_auth_token=token)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", use_auth_token=token)\n",
    "\n",
    "model.eval()  # eval mode disables dropout\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Everyone is \"\n",
    "encoded = tokenizer.encode(prompt, return_tensors=\"pt\")  # shape: [1, seq_len]\n",
    "\n",
    "# Generate text using greedy decoding\n",
    "max_new_tokens = 20\n",
    "generated = encoded\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(generated)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "# Decode to readable text\n",
    "decoded_text = tokenizer.decode(generated[0].tolist())\n",
    "print(\"Generated text:\\n\", decoded_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
