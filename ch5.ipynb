{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62343b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from basic import create_dataloader_v1\n",
    "from GPT_architecture import GPTModel\n",
    "from GPT_architecture import Generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c7e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 256, # Shortened from 1024 to make it easier to train on a laptop\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(txt, tokenizer):\n",
    "    encoded= tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor= torch.tensor(encoded).unsqueeze(0) # add dimension at position 0\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2e04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_txt(token_ids, tokenizer):\n",
    "    flat= token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(data_loader, model, device, num_batches=None):\n",
    "    ip_batch= ip_batch.to(device)\n",
    "    target_batch= target_batch.to(device)\n",
    "    logits= model(ip_batch)\n",
    "    loss= torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss=0\n",
    "    if len(data_loader)==0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches= min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss= cross_entropy(input_batch, target_batch, model, device)\n",
    "            total_loss+=loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # to stop dropout\n",
    "    # no grad to stop gradient calculation \n",
    "    with torch.no_grad(): \n",
    "        train_loss= cross_entropy(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss= cross_entropy(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size= model.pos_emb.weight.shape[0]\n",
    "    encoded= text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids= Generate_text(model= model, idx= encoded, max_new_tokens=50, context_size=context_size)\n",
    "        decoded_text= token_ids_to_txt(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77228bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_fr, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen= [], [], []\n",
    "    tokens_seen, global_step= 0,-1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "         optimizer.zero_grad()\n",
    "         loss = cross_entropy(input_batch, target_batch, model, device)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         tokens_seen+=input_batch.numel()\n",
    "         global_step+=1\n",
    "\n",
    "         if global_step % eval_fr ==0:\n",
    "            train_loss, val_loss = evaulate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f\"Ep{epoch+1}(Step {global_step:06d}):\"\n",
    "                  f\"Train loss{train_loss:.3f}\"\n",
    "                  f\"Val loss{val_loss:.3f}\")\n",
    "        \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_token_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond= idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits= model(idx_cond)\n",
    "        logits= logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits, _=torch.topk(logits, top_k)\n",
    "            min_val= top_logits[:,-1]\n",
    "            logits= torch.where(\n",
    "                logits<min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits= logits/temperature\n",
    "            probs= torch.softmax(logits, dim=-1)\n",
    "            idx_next= torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next= torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx= torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d474a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.manual_seed(123)\n",
    "    model= GPTModel(GPT_CONFIG_124M)\n",
    "    model.eval()\n",
    "\n",
    "    start_context= \"every effort moves you\"\n",
    "    tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "    token_ids= Generate_text(\n",
    "        model= model,\n",
    "        idx= text_to_token_ids(start_context, tokenizer),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M['context_length']\n",
    "    )\n",
    "    print(\"Output text\", token_ids_to_txt(token_ids, tokenizer))\n",
    "\n",
    "    # Calculate loss\n",
    "    inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "                           [40, 1107, 588]])    # \"I really like\"]\n",
    "    \n",
    "    targets = torch.tensor([[3626, 6100, 345],  # [\" effort moves you\",\n",
    "                            [588, 428, 11311]]) # \" really like chocholate\"]\n",
    "    with torch.no_grad():\n",
    "        logits= model(inputs)\n",
    "    probas= torch.softmax(logits, dim=-1)\n",
    "    print('Probability received for next word:', probas.shape)\n",
    "\n",
    "    token_ids= torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "\n",
    "    print(f\"Target batch 1: {token_ids_to_txt(targets[0], tokenizer)}\")\n",
    "    print(f\"Output batch 1:\" f\"{token_ids_to_txt(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "    text_idx=0\n",
    "    target_probas_1= probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "    print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "    text_idx = 1\n",
    "    target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "    print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "    log_probas= torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "    print(\"Log_probas:\", log_probas)\n",
    "\n",
    "    avg_log_probas= torch.mean(log_probas)\n",
    "    print(\"Average log probas\", avg_log_probas)\n",
    "\n",
    "    neg_avg_log_probas= avg_log_probas*-1\n",
    "    print(\"Negative acg log probas\", neg_avg_log_probas)\n",
    "\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    print(\"target shape:\", targets.shape)\n",
    "\n",
    "    logits_flat= logits.flatten(0,1)\n",
    "    targets_flat= targets.flatten()\n",
    "    print(\"Flattened logits:\", logits_flat.shape)\n",
    "    print(\"Flattened targets\", targets_flat.shape)\n",
    "\n",
    "    loss= torch.nn.functional.binary_cross_entropy(logits.flat, targets_flat)\n",
    "    print(\"loss\", loss)\n",
    "\n",
    "    perplexity= torch.exp(loss)\n",
    "    print(f\"Perplexity {perplexity}\")\n",
    "\n",
    "    filepath=\n",
    "    with open (filepath, 'r', encoding='utf-8') as file:\n",
    "        text_data= file.read()\n",
    "    \n",
    "    total_characters= len(text_data)\n",
    "    total_tokens= len(tokenizer.encode(text_data))\n",
    "    print(\"Characters:\", total_characters)\n",
    "    print(\"Tokens\", total_tokens)\n",
    "\n",
    "    train_ratio=0.90\n",
    "    split_idx= int(train_ratio*len(text_data))\n",
    "    train_data= text_data[:split_idx]\n",
    "    val_data= text_data[split_idx]\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    train_loader= create_dataloader_v1(\n",
    "        train_data,\n",
    "        batch_size=2,\n",
    "        max_len=GPT_CONFIG_124M['context_len'],\n",
    "        stride= GPT_CONFIG_124M[\"context_len\"],\n",
    "        drop_last= True,\n",
    "        shuffle= True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    val_loader = create_dataloader_v1(\n",
    "            val_data,\n",
    "            batch_size=2, # A more common batch size would be 1024, this is just for the demonstration purpose\n",
    "            max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "            stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Iterate through the data loaders to check that they were created correctly\n",
    "    \n",
    "    print(\"Train loader:\")\n",
    "    for x, y in train_loader:\n",
    "        print(x.shape, y.shape)\n",
    "    \n",
    "    print(\"\\nValidation loader:\")\n",
    "    for x, y in val_loader:\n",
    "        print(x.shape, y.shape)\n",
    "    \n",
    "    device= torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        train_loss= calc_loss_loader(train_loader, model, device)\n",
    "        val_loss= calc_loss_loader(val_loader, model, device)\n",
    "     \n",
    "    print(\"Training loss:\", train_loss)\n",
    "    print(\"Val loss\", val_loss)\n",
    "\n",
    "    # Let's pretrain :)\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model= GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer= torch.optim.AdamW(\n",
    "        model.parameters(), lr=0.0004, weight_decay=0.1\n",
    "    )    \n",
    "    num_epochs= 10\n",
    "    print(\"LET'S TRAIN\")\n",
    "    train_losses, val_losses, tokens_seen= train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_fr=5, eval_iter=1, start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "    )\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "    \n",
    "    print(\"epochs tensor:\", epochs_tensor)\n",
    "    print(\"tokens seen:\", tokens_seen)\n",
    "    print(\"train losses:\", train_losses)\n",
    "    print(\"val losses:\", val_losses)\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "    token_ids= Generate_text(model=model, idx= text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "                            max_new_tokens=25,\n",
    "                            context_size=GPT_CONFIG_124M[\"context_len\"]\n",
    "                             )\n",
    "    \n",
    "    torch.manual_seed(123)\n",
    "    token_ids = generate(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "            max_new_tokens=15,\n",
    "            context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "            top_k=25,\n",
    "            temperature=1.4\n",
    "    )\n",
    "    print(\"Output text:\\n\", token_ids_to_txt(token_ids, tokenizer))\n",
    "    \n",
    "    print(\"Saving the pretrained model\")\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    \n",
    "    print(\"Loading just saved model\")\n",
    "    \n",
    "    model_loaded = GPTModel(GPT_CONFIG_124M)\n",
    "    model_loaded.load_state_dict(torch.load(\"model.pth\"))\n",
    "    model_loaded.eval()\n",
    "    \n",
    "    token_ids = generate(\n",
    "            model=model_loaded,\n",
    "            idx=text_to_token_ids(\"It was another Thursday in the office\", tokenizer),\n",
    "            max_new_tokens=15,\n",
    "            context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "            top_k=1,\n",
    "            temperature=1.,\n",
    "    )\n",
    "    print(\"Output text:\\n\", token_ids_to_txt(token_ids, tokenizer))\n",
    "    \n",
    "    # Saving model together with optimizer to continue training from that step\n",
    "    \n",
    "    print(\"Saving model together with the optimizer now\")\n",
    "    torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        },\n",
    "        \"model_and_optimizer.pth\"\n",
    "    ) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
