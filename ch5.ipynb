{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62343b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from basic import create_dataloader_v1\n",
    "from GPT_architecture import GPTModel\n",
    "from GPT_architecture import Generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c7e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 256, # Shortened from 1024 to make it easier to train on a laptop\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(txt, tokenizer):\n",
    "    encoded= tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor= torch.tensor(encoded).unsqueeze(0) # add dimension at position 0\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2e04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_txt(token_ids, tokenizer):\n",
    "    flat= token_ids.squeeze(0) # Remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(data_loader, model, device, num_batches=None):\n",
    "    ip_batch= ip_batch.to(device)\n",
    "    target_batch= target_batch.to(device)\n",
    "    logits= model(ip_batch)\n",
    "    loss= torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss=0\n",
    "    if len(data_loader)==0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches= min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss= cross_entropy(input_batch, target_batch, model, device)\n",
    "            total_loss+=loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() # to stop dropout\n",
    "    # no grad to stop gradient calculation \n",
    "    with torch.no_grad(): \n",
    "        train_loss= cross_entropy(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss= cross_entropy(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size= model.pos_emb.weight.shape[0]\n",
    "    encoded= text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids= Generate_text(model= model, idx= encoded, max_new_tokens=50, context_size=context_size)\n",
    "        decoded_text= token_ids_to_txt(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77228bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_fr, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen= [], [], []\n",
    "    tokens_seen, global_step= 0,-1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "         optimizer.zero_grad()\n",
    "         loss = cross_entropy(input_batch, target_batch, model, device)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         tokens_seen+=input_batch.numel()\n",
    "         global_step+=1\n",
    "\n",
    "         if global_step % eval_fr ==0:\n",
    "            train_loss, val_loss = evaulate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f\"Ep{epoch+1}(Step {global_step:06d}):\"\n",
    "                  f\"Train loss{train_loss:.3f}\"\n",
    "                  f\"Val loss{val_loss:.3f}\")\n",
    "        \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_token_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond= idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits= model(idx_cond)\n",
    "        logits= logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits, _=torch.topk(logits, top_k)\n",
    "            min_val= top_logits[:,-1]\n",
    "            logits= torch.where(\n",
    "                logits<min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits= logits/temperature\n",
    "            probs= torch.softmax(logits, dim=-1)\n",
    "            idx_next= torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next= torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx= torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d474a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
