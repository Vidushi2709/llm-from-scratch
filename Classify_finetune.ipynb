{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8627efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import matplotlib.pyplot as plt\n",
    "from ch5 import GPTModel\n",
    "from ch5_part2 import load_weights_into_gpt\n",
    "from GPT_architecture import Generate_text\n",
    "from ch5 import text_to_token_ids, token_ids_to_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc850c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SMSSpamCollection.tsv\", sep=\"\\t\", header=None)\n",
    "df.columns = [\"label\", \"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c59cdf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2351bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the classes\n",
    "def create_balanced_dataset(df):\n",
    "    num_spam= df[df['Label'] == \"spam\"].shape[0]\n",
    "    ham_subset= df[df['Label'] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7199db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad all the dataset to have the same length -> we padded all the sentences to be as long as the longest sentence of the dataset\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        self.encoded_texts = [\n",
    "                tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "            self.encoded_texts = [\n",
    "                    encoded_text[:self.max_length]\n",
    "                    for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        self.encoded_texts = [\n",
    "                encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "                for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "                torch.tensor(encoded, dtype=torch.long),\n",
    "                torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2924d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_batch)[:, -1, :]\n",
    "        predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        num_examples += predicted_labels.shape[0]\n",
    "        correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f266ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :] # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc0cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536854f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss= calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss= calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e9c938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, device, num_epochs, eval_fre, eval_iter, tokenizer):\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies= [],[],[],[]\n",
    "    examples_seen, global_steps= 0,-1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss= calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen+=input_batch.shape[0]\n",
    "            global_steps+=1\n",
    "\n",
    "            if global_steps%eval_fre==0:\n",
    "                train_loss, val_loss= evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep{epoch+1} (Step {global_steps:06d})\"\n",
    "                      f\"train loss {train_loss:.3f}, val loss {val_loss:.3f}\")\n",
    "        train_accuracy= calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy= calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy:{train_accuracy*100:.2f}Z% |\", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy) \n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d082f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0) # Invinsible\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c09551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_len=None, pad_token_ids=50256):\n",
    "    model.eval()\n",
    "\n",
    "    input_ids= tokenizer.encode(text)\n",
    "    supported_context_len= model.pos_emb.weight.shape[1]\n",
    "    input_ids= input_ids[:min(max_len, supported_context_len)]\n",
    "\n",
    "    input_ids+=[pad_token_ids]*(max_len-len(input_ids))\n",
    "    input_tensor= torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits= model(input_tensor)[:,-1,:]\n",
    "    predicted_labels= torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"You are getting scammed girlie pop\" if predicted_labels==1 else \"not a spam girlie pop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e88da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Downloading file\n",
    "    data_file_path= \"SMSSpamCollection.tsv\"\n",
    "\n",
    "    # Loading it into panda dataframes\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "    print(df[\"Label\"].value_counts())\n",
    "\n",
    "    # Get balanced dataset for finetuning\n",
    "    balanced_df = create_balanced_dataset(df)\n",
    "    print(\"Balanced dataset value counts:\\n\", balanced_df[\"Label\"].value_counts())\n",
    "\n",
    "    # Convert string class labels to 0 and 1\n",
    "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "    # 70% for training, 10% for validation adn 20% for testing \n",
    "    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "    # Save to csv to re-use later\n",
    "    train_df.to_csv(\"train.csv\", index=None)\n",
    "    validation_df.to_csv(\"validation.csv\", index=None)\n",
    "    test_df.to_csv(\"test.csv\", index=None)\n",
    "\n",
    "    # Verifying which is <|endoftext|> token for padding\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    print(\"<|endoftext|> token id is \", tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n",
    "\n",
    "    # Creating data sets\n",
    "    train_dataset = SpamDataset(\n",
    "            csv_file=\"train.csv\",\n",
    "            max_length=None,\n",
    "            tokenizer = tokenizer\n",
    "    )\n",
    "    print(\"Longest sequence in the dataset is \", train_dataset.max_length)\n",
    "    val_dataset = SpamDataset(\n",
    "            csv_file=\"validation.csv\",\n",
    "            max_length=train_dataset.max_length,\n",
    "            tokenizer=tokenizer\n",
    "    )\n",
    "    test_dataset = SpamDataset(\n",
    "            csv_file=\"test.csv\",\n",
    "            max_length=train_dataset.max_length,\n",
    "            tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Creating dataloaders\n",
    "    num_workers = 0\n",
    "    batch_size = 8\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "            dataset=val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "    )\n",
    "\n",
    "    # Ensure data loaders are working correctly, iterate over the training loader and print tensor dimensions of the last batch\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        pass\n",
    "    print(\"Input batch dimensions:\", input_batch.shape)\n",
    "    print(\"Label batch dimensions:\", target_batch.shape)\n",
    "\n",
    "    print(f\"{len(train_loader)} training batches\")\n",
    "    print(f\"{len(val_loader)} validation batches\")\n",
    "    print(f\"{len(test_loader)} test batches\")\n",
    "\n",
    "    # 6.4 Initialize a model with pretrained weights\n",
    "    CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "    INPUT_PROMPT = \"Every effort moves\"\n",
    "    BASE_CONFIG = {\n",
    "            \"vocab_size\": 50257,     # Vocabulary size\n",
    "            \"context_length\": 1024,  # Context length\n",
    "            \"drop_rate\": 0.0,        # Dropout rate\n",
    "            \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "    model_configs = {\n",
    "            \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "            \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "            \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "            \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "    assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "            f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "            f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "            f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    "    )\n",
    "\n",
    "    model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "    \n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    load_weights_into_gpt(model, params)\n",
    "    model.eval()\n",
    "    \n",
    "    # Model is loaded, lets ensure it generate coherent text\n",
    "    text_1 = \"Every effort moves you\"\n",
    "    token_ids = Generate_text(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(text_1, tokenizer),\n",
    "            max_new_tokens=15,\n",
    "            context_size=BASE_CONFIG[\"context_length\"]\n",
    "    )\n",
    "    print(token_ids_to_txt(token_ids, tokenizer))\n",
    "\n",
    "    # Lets see if the model can classify spam messages by prompting it with instructions\n",
    "    text_2 = (\n",
    "            \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "            \" 'You are a winner you have been specially\"\n",
    "            \" selected to receive $1000 cash or a $2000 award.'\"\n",
    "    )\n",
    "    token_ids = Generate_text(\n",
    "            model=model,\n",
    "            idx=text_to_token_ids(text_2, tokenizer),\n",
    "            max_new_tokens=23,\n",
    "            context_size=BASE_CONFIG[\"context_length\"]\n",
    "    )\n",
    "    print(token_ids_to_txt(token_ids, tokenizer))\n",
    "\n",
    "    # 6.5 Onto modifying the pre-trained model\n",
    "    # Let's first print the current state\n",
    "    print(model)\n",
    "\n",
    "    # Freeze the model, make the layers non-trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace output head\n",
    "    torch.manual_seed(123)\n",
    "    num_classes = 2\n",
    "    model.out_head = torch.nn.Linear(\n",
    "            in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "            out_features=num_classes\n",
    "    )\n",
    "    # Make the last LayerNorm and Transformer block trainable\n",
    "    for param in model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    inputs = tokenizer.encode(\"Do you have time\")\n",
    "    inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    print(\"Outputs:\\n\", outputs)\n",
    "    print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)\n",
    "    print(\"Last output token:\", outputs[:, -1, :])\n",
    "\n",
    "    # Obtaining the class label for that example\n",
    "    logits = outputs[:, -1, :]\n",
    "    label = torch.argmax(logits)\n",
    "    print(\"Class label:\", label.item())\n",
    "\n",
    "    # Test out classification accuracies\n",
    "    device = torch.device(\"cpu\") # torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "    val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "    test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "    print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Computing initial loss for each data set\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "        test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "    print(\"The initial loss values are as follows:\\n\")\n",
    "    print(f\"Training loss: {train_loss:.3f}\")\n",
    "    print(f\"Validation loss: {val_loss:.3f}\")\n",
    "    print(f\"Test loss: {test_loss:.3f}\")\n",
    "\n",
    "    # Time to train! 6.7\n",
    "    start_time = time.time()\n",
    "    torch.manual_seed(123)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "    num_epochs = 5\n",
    "\n",
    "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "            model, train_loader, val_loader, optimizer, device,\n",
    "            num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "            tokenizer=tokenizer\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "    # Plotting what we got\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "    #plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n",
    "\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "    #plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")\n",
    "\n",
    "    # Now evaluate accuracy for the entire dataset\n",
    "    train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "    val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "    test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "    print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "    text_1 = (\n",
    "            \"You are a winner you have been specially\"\n",
    "            \" selected to receive $1000 cash or a $2000 award.\"\n",
    "    )\n",
    "    print(classify_review(\n",
    "        text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "    ))\n",
    "\n",
    "    text_2 = (\n",
    "            \"Hey, just wanted to check if we're still on\"\n",
    "            \" for dinner tonight? Let me know!\"\n",
    "    )\n",
    "    print(classify_review(\n",
    "        text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "    ))\n",
    "\n",
    "    # Saving model for later re-use without having to train\n",
    "    torch.save(model.state_dict(), \"review_classifier.pth\")\n",
    "\n",
    "    # Once saved the model can be loaded as follows\n",
    "    model_state_dict = torch.load(\"review_classifier.pth\")\n",
    "    model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa671bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset value counts:\n",
      " Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n",
      "<|endoftext|> token id is  [50256]\n",
      "Longest sequence in the dataset is  120\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions: torch.Size([8])\n",
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 13.1kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 350kiB/s] \n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 12.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001:  10%|▉         | 49.6M/498M [03:58<35:58, 208kiB/s]    \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 683kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:07<00:00, 61.8kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:02<00:00, 167kiB/s]  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Read fewer bytes than requested",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 105\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m BASE_CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dataset\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeds model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Reinitialize data sets with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`max_length=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m model_size \u001b[38;5;241m=\u001b[39m CHOOSE_MODEL\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m settings, params \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_and_load_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(BASE_CONFIG)\n\u001b[0;32m    108\u001b[0m load_weights_into_gpt(model, params)\n",
      "File \u001b[1;32mc:\\Users\\Vidushi\\OneDrive\\Desktop\\LLM\\gpt_download.py:44\u001b[0m, in \u001b[0;36mdownload_and_load_gpt2\u001b[1;34m(model_size, models_dir)\u001b[0m\n\u001b[0;32m     42\u001b[0m tf_ckpt_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlatest_checkpoint(model_dir)\n\u001b[0;32m     43\u001b[0m settings \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 44\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mload_gpt2_params_from_tf_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_ckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings, params\n",
      "File \u001b[1;32mc:\\Users\\Vidushi\\OneDrive\\Desktop\\LLM\\gpt_download.py:138\u001b[0m, in \u001b[0;36mload_gpt2_params_from_tf_ckpt\u001b[1;34m(ckpt_path, settings)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Iterate over each variable in the checkpoint\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlist_variables(ckpt_path):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# Load the variable and remove singleton dimensions\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     variable_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Process the variable name to extract relevant parts\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     variable_name_parts \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the 'model/' prefix\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py:114\u001b[0m, in \u001b[0;36mload_variable\u001b[1;34m(ckpt_dir_or_file, name)\u001b[0m\n\u001b[0;32m    112\u001b[0m   name \u001b[38;5;241m=\u001b[39m name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    113\u001b[0m reader \u001b[38;5;241m=\u001b[39m load_checkpoint(ckpt_dir_or_file)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:66\u001b[0m, in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the tensor from the Checkpoint object.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointReader_GetTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mIndexError\u001b[0m: Read fewer bytes than requested"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
